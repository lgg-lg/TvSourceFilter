# modules/module2_combine.py
import pandas as pd
import requests
import os
import logging
from urllib.parse import urlparse
import urllib.request
import html
import opencc
import re

# --- é…ç½®æ—¥å¿— ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def read_txt_to_array(file_name):
    try:
        with open(file_name, 'r', encoding='utf-8') as file:
            lines=[]
            for line in file:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                lines.append(line)
            return lines
    except FileNotFoundError:
        print(f"File '{file_name}' not found.")
        return []
    except Exception as e:
        print(f"An error occurred: {e}")
        return []

#ç®€ç¹è½¬æ¢
def traditional_to_simplified(text: str) -> str:
    # åˆå§‹åŒ–è½¬æ¢å™¨ï¼Œ"t2s" è¡¨ç¤ºä»ç¹ä½“è½¬ä¸ºç®€ä½“
    converter = opencc.OpenCC('t2s')
    simplified_text = converter.convert(text)
    return simplified_text
#M3Uæ ¼å¼åˆ¤æ–­
def is_m3u_content(text):
    lines = text.splitlines()
    first_line = lines[0].strip()
    return first_line.startswith("#EXTM3U")
    
def convert_m3u_to_txt(m3u_content):
    # åˆ†è¡Œå¤„ç†
    lines = m3u_content.split('\n')
    
    # ç”¨äºå­˜å‚¨ç»“æœçš„åˆ—è¡¨
    txt_lines = []
    
    # ä¸´æ—¶å˜é‡ç”¨äºå­˜å‚¨é¢‘é“åç§°
    channel_name = ""
    
    for line in lines:
        # è¿‡æ»¤æ‰ #EXTM3U å¼€å¤´çš„è¡Œ
        if line.startswith("#EXTM3U"):
            continue
        # å¤„ç† #EXTINF å¼€å¤´çš„è¡Œ
        if line.startswith("#EXTINF"):
            # è·å–é¢‘é“åç§°ï¼ˆå‡è®¾é¢‘é“åç§°åœ¨å¼•å·åï¼‰
            channel_name = line.split(',')[-1].strip()
        # å¤„ç† URL è¡Œ
        elif line.startswith("http") or line.startswith("rtmp") or line.startswith("p3p") :
            txt_lines.append(f"{channel_name},{line.strip()}")
        
        # å¤„ç†åç¼€åä¸ºm3uï¼Œä½†æ˜¯å†…å®¹ä¸ºtxtçš„æ–‡ä»¶
        if "#genre#" not in line and "," in line and "://" in line:
            # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼ï¼ŒåŒ¹é…é¢‘é“åç§°,URL çš„æ ¼å¼ï¼Œå¹¶ç¡®ä¿ URL åŒ…å« "://"
            # xxxx,http://xxxxx.xx.xx
            pattern = r'^[^,]+,[^\s]+://[^\s]+$'
            if bool(re.match(pattern, line)):
                txt_lines.append(line)
    
    # å°†ç»“æœåˆå¹¶æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä»¥æ¢è¡Œç¬¦åˆ†éš”
    return '\n'.join(txt_lines)
# æ·»åŠ channel_nameå‰å‰”é™¤éƒ¨åˆ†ç‰¹å®šå­—ç¬¦
removal_list = ["ã€ŒIPV4ã€","ã€ŒIPV6ã€","[ipv6]","[ipv4]","_ç”µä¿¡", "ç”µä¿¡","ï¼ˆHDï¼‰","[è¶…æ¸…]","é«˜æ¸…","è¶…æ¸…", "-HD","(HK)","AKtv","@","IPV6","ğŸï¸","ğŸ¦"," ","[BD]","[VGA]","[HD]","[SD]","(1080p)","(720p)","(480p)"]
def clean_channel_name(channel_name, removal_list):
    for item in removal_list:
        channel_name = channel_name.replace(item, "")
        channel_name = channel_name.replace("CCTV-", "CCTV")
        channel_name = channel_name.replace("CCTV0","CCTV")
        channel_name = channel_name.replace("PLUS", "+")
        channel_name = channel_name.replace("NewTV-", "NewTV")
        channel_name = channel_name.replace("iHOT-", "iHOT")
        channel_name = channel_name.replace("NEW", "New")
        channel_name = channel_name.replace("New_", "New")
    return channel_name
# å¤„ç†å¸¦$çš„URLï¼ŒæŠŠ$ä¹‹åçš„å†…å®¹éƒ½å»æ‰ï¼ˆåŒ…æ‹¬$ä¹Ÿå»æ‰ï¼‰ ã€2024-08-08 22:29:11ã€‘
def clean_url(url):
    last_dollar_index = url.rfind('$')  # å®‰å…¨èµ·è§æ‰¾æœ€åä¸€ä¸ª$å¤„ç†
    if last_dollar_index != -1:
        return url[:last_dollar_index]
    return url
    
def process_channel_line(line):
    if  "#genre#" not in line and "#EXTINF:" not in line and "," in line and "://" in line:
        channel_name = line.split(',')[0]
        channel_name = traditional_to_simplified(channel_name)  #ç¹è½¬ç®€
        channel_name = clean_channel_name(channel_name, removal_list)  #åˆ†å‘å‰æ¸…ç†channel_nameä¸­ç‰¹å®šå­—ç¬¦
        channel_address = clean_url(line.split(',')[1]).strip()  #æŠŠURLä¸­$ä¹‹åçš„å†…å®¹éƒ½å»æ‰
        line=channel_name+","+channel_address #é‡æ–°ç»„ç»‡line
    return line

def process_url(url):
    logger.info(f"å¤„ç†URL: {url}")
    result = []
    try:
        #other_lines.append(url+",#genre#")  # å­˜å…¥other_linesä¾¿äºcheck 2024-08-02 10:41
        
        # åˆ›å»ºä¸€ä¸ªè¯·æ±‚å¯¹è±¡å¹¶æ·»åŠ è‡ªå®šä¹‰header
        headers = {
            'User-Agent': 'PostmanRuntime-ApipostRuntime/1.1.0',
        }
        req = urllib.request.Request(url, headers=headers)
        # æ‰“å¼€URLå¹¶è¯»å–å†…å®¹
        with urllib.request.urlopen(req,timeout=10) as response:
            # ä»¥äºŒè¿›åˆ¶æ–¹å¼è¯»å–æ•°æ®
            data = response.read()
            # å°†äºŒè¿›åˆ¶æ•°æ®è§£ç ä¸ºå­—ç¬¦ä¸²
            try:
                # å…ˆå°è¯• UTF-8 è§£ç 
                text = data.decode('utf-8')
            except UnicodeDecodeError:
                try:
                    # è‹¥ UTF-8 è§£ç å¤±è´¥ï¼Œå°è¯• GBK è§£ç 
                    text = data.decode('gbk')
                except UnicodeDecodeError:
                    try:
                        # è‹¥ GBK è§£ç å¤±è´¥ï¼Œå°è¯• ISO-8859-1 è§£ç 
                        text = data.decode('iso-8859-1')
                    except UnicodeDecodeError:
                        print("æ— æ³•ç¡®å®šåˆé€‚çš„ç¼–ç æ ¼å¼è¿›è¡Œè§£ç ã€‚")
                        
            #å¤„ç†m3uæå–channel_nameå’Œchannel_address
            if is_m3u_content(text):
                text=convert_m3u_to_txt(text)

            # é€è¡Œå¤„ç†å†…å®¹
            lines = text.split('\n')
            print(f"è¡Œæ•°: {len(lines)}")
            for index, line in enumerate(lines):
                if index % 5000 == 0 and index > 0:  # æ¯5000è¡Œæç¤ºä¸€æ¬¡ï¼ˆè·³è¿‡ç¬¬0è¡Œï¼‰
                    print(f"å·²å¤„ç† {index} è¡Œ...")

                line = line.strip()
                if not line:
                    continue
                if  "#genre#" not in line and "," in line and "://" in line and not line.startswith('#'):
                    # æ‹†åˆ†æˆé¢‘é“åå’ŒURLéƒ¨åˆ†
                    channel_name, channel_address = line.split(',', 1)
                    #éœ€è¦åŠ å¤„ç†å¸¦#å·æº=äºˆåŠ é€Ÿæº
                    if "#" not in channel_address:
                        processedline=process_channel_line(line) # å¦‚æœæ²¡æœ‰äº•å·ï¼Œåˆ™ç…§å¸¸æŒ‰ç…§æ¯è¡Œè§„åˆ™è¿›è¡Œåˆ†å‘
                        name, url = processedline.split(',', 1)
                        result.append([name, url.strip(), ''])
                    else: 
                        # å¦‚æœæœ‰â€œ#â€å·ï¼Œåˆ™æ ¹æ®â€œ#â€å·åˆ†éš”
                        url_list = channel_address.split('#')
                        for channel_url in url_list:
                            newline=f'{channel_name},{channel_url}'
                            processedline=process_channel_line(newline)
                            name, url = processedline.split(',', 1)
                            result.append([name, url.strip(), ''])
            return result
    except Exception as e:
        print(f"å¤„ç†URLæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
def process_local(url):
    logger.info(f"å¤„ç†URL: {url}")
    result = []
    try:
        if os.path.exists(url):
            with open(url, 'r', encoding='utf-8') as file:
                text = file.read()
            # é€è¡Œå¤„ç†å†…å®¹
            lines = text.split('\n')
            print(f"è¡Œæ•°: {len(lines)}")
            for index, line in enumerate(lines):
                if index % 1000 == 0 and index > 0:  # æ¯1000è¡Œæç¤ºä¸€æ¬¡ï¼ˆè·³è¿‡ç¬¬0è¡Œï¼‰
                    print(f"å·²å¤„ç† {index} è¡Œ...")

                line = line.strip()
                if not line:
                    continue
                if  "#genre#" not in line and "," in line and "://" in line and not line.startswith('#'):
                    # æ‹†åˆ†æˆé¢‘é“åå’ŒURLéƒ¨åˆ†
                    channel_name, channel_address = line.split(',', 1)
                    #éœ€è¦åŠ å¤„ç†å¸¦#å·æº=äºˆåŠ é€Ÿæº
                    if "#" not in channel_address:
                        processedline=process_channel_line(line) # å¦‚æœæ²¡æœ‰äº•å·ï¼Œåˆ™ç…§å¸¸æŒ‰ç…§æ¯è¡Œè§„åˆ™è¿›è¡Œåˆ†å‘
                        name, url = processedline.split(',', 1)
                        result.append([name, url.strip(), ''])
                    else: 
                        # å¦‚æœæœ‰â€œ#â€å·ï¼Œåˆ™æ ¹æ®â€œ#â€å·åˆ†éš”
                        url_list = channel_address.split('#')
                        for channel_url in url_list:
                            newline=f'{channel_name},{channel_url}'
                            processedline=process_channel_line(newline)
                            name, url = processedline.split(',', 1)
                            result.append([name, url.strip(), ''])
            return result
    except Exception as e:
        print(f"å¤„ç†URLæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")


def deduplicate(data):
    df = pd.DataFrame(data, columns=['name', 'url', 'extra'])
    df['url'] = df['url'].str.replace(r'[\r\n,;"\'\t]', '', regex=True)
    df['url'] = df['url'].apply(lambda x: html.unescape(x) if isinstance(x, str) else x)
    df['name'] = df['name'].str.replace(r'[\r\n,;"\'\t]', '', regex=True)
    df['extra'] = df['extra'].str.replace(r'[\r\n,;"\'\t]', '', regex=True)
    df['extra'] = df['extra'].apply(lambda x: html.unescape(x) if isinstance(x, str) else x)
    df.drop_duplicates(subset=['name', 'url'], keep='first', inplace=True)
    return df

# def save_df(df, path):
#     logger.info(f"æœ‰{len(df)}æ¡æ•°æ®å¾…å†™å‡º")
#     df.to_csv(path, index=False, header=False, encoding='utf-8')
#     logger.info(f"æ•°æ®å·²å†™å‡ºåˆ° {path}")

def save_df(df, path):
    chunk_size = 50000
    total_rows = len(df)
    logger.info(f"æœ‰ {total_rows} æ¡æ•°æ®å¾…å†™å‡º")

    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤ï¼ˆç¡®ä¿ä»å¤´å¼€å§‹å†™ï¼‰
    if os.path.exists(path):
        os.remove(path)
        logger.info(f"å·²åˆ é™¤æ—§æ–‡ä»¶ {path}")

    # åˆ†å—å†™å…¥
    num_chunks = (total_rows + chunk_size - 1) // chunk_size  # å‘ä¸Šå–æ•´
    for i in range(num_chunks):
        start_row = i * chunk_size
        end_row = min(start_row + chunk_size, total_rows)
        chunk = df.iloc[start_row:end_row]

        # ç¬¬ä¸€å—å†™å…¥æ—¶ï¼Œä¸å†™ headerï¼ˆå› ä¸º header=Falseï¼‰ï¼›åç»­å—è¿½åŠ å†™å…¥
        write_header = False  # ä½ åŸéœ€æ±‚æ˜¯ header=Falseï¼Œæ‰€ä»¥å§‹ç»ˆä¸å†™ header
        write_mode = 'w' if i == 0 else 'a'  # ç¬¬ä¸€å—è¦†ç›–å†™ï¼Œåç»­å—è¿½åŠ å†™

        chunk.to_csv(
            path,
            index=False,
            header=write_header,
            encoding='utf-8',
            mode=write_mode
        )

        logger.info(f"æ•°æ®åˆ†æ®µ {i+1}/{num_chunks} å·²è¿½åŠ å†™å…¥åˆ° {path}")

    logger.info(f"å…¨éƒ¨æ•°æ®å·²å†™å‡ºåˆ° {path}")

def combine_sources():
    logger.info("å¼€å§‹æ‰§è¡Œæ¨¡å—2ï¼šè¯»å–è®¢é˜…æº")
    urls=read_txt_to_array(os.path.join("config", "subscribe.txt"))
    net_data=[]
    for url in urls:
        if url.startswith("http"):        
            net_data+=process_url(url)
    net_df = deduplicate(net_data)

    # è¯»å–æœ¬åœ°æº
    result_data = process_local(os.path.join("config", "user_result.txt"))
    local_data = process_local(os.path.join("config", "localsource.txt"))
    own_data = process_local(os.path.join("output", "ownsource.txt"))
    errorflag=False

    
    try:
        save_df(net_df, os.path.join("output", "netsource.txt"))
    except Exception as e: 
        logger.error(f"ç½‘ç»œæºå†™å‡ºå¤±è´¥:{e}")
        errorflag=True
        with open(os.path.join("output", "netsource_log.csv"), 'w', encoding='gbk', errors='replace') as f:
            for _, row in net_df.iterrows():
                f.write(f"{row['name']},{row['url']},{row['extra']}\n")
        
    
    # åˆå¹¶æ‰€æœ‰æº
    if errorflag:
        all_data = result_data + local_data + own_data
    else:
        all_data = result_data + local_data + own_data + net_data
        
    all_df = deduplicate(all_data)
    all_df.drop(columns=['extra'], inplace=True)
    try:
        save_df(all_df, os.path.join("output", "allsource.txt"))
    except Exception as e: 
        logger.error(f"ç½‘ç»œæºå†™å‡ºå¤±è´¥:{e}")
        with open(os.path.join("output", "allsource_log.txt"), 'w', encoding='gbk', errors='replace') as f:
            for _, row in all_df.iterrows():
                f.write(f"{row['name']},{row['url']}\n")
    logger.info("æ¨¡å—2æ‰§è¡Œå®Œæ¯•")
    return errorflag
if __name__ == '__main__':

    combine_sources()


# import pandas as pd
# import requests
# import os
# import logging
# from urllib.parse import urlparse

# # --- é…ç½®æ—¥å¿— ---
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
# logger = logging.getLogger(__name__)

# def read_sources(file_path):
#     data = []
#     if not os.path.exists(file_path):
#         return data
#     with open(file_path, 'r', encoding='utf-8') as f:
#         for line in f:
#             line = line.strip()
#             if not line or line.startswith('#') or ',#genre#' in line:
#                 continue
#             parts = line.split(',', 1)
#             name = parts[0].strip()
#             url = parts[1].strip() if len(parts) > 1 else ''
#             extra = ''
#             if '$' in url:
#                 url, extra = url.split('$', 1)
#             data.append([name, url.strip(), extra.strip()])
#     return data

# def read_subscribe_sources(subscribe_path):
#     data = []
#     with open(subscribe_path, 'r', encoding='utf-8') as f:
        
#         for line in f:
#             line = line.strip()
#             if not line or line.startswith('#'):
#                 continue
#             try:
#                 response = requests.get(line, timeout=10)
#                 if 'm3u' in urlparse(line).path.lower():
#                     logger.info(f"æ£€æµ‹åˆ°M3Uæº{line}ï¼Œå³å°†å¤„ç†")
#                     # M3U æ ¼å¼è§£æ
#                     lines = response.text.splitlines()
#                     i = 0
#                     while i < len(lines):
#                         if lines[i].startswith('#EXTINF'):
#                             info = lines[i]
#                             url = lines[i+1]
#                             name = info.split(',')[-1].strip()
#                             data.append([name, url.strip(), ''])
#                             i += 2
#                         else:
#                             i += 1
#                 else:
#                     # TXT æ ¼å¼è§£æ
#                     for line in response.text.splitlines():
#                         line = line.strip()
#                         if not line or line.startswith('#') or ',#genre#' in line:
#                             continue
#                         if '\t' in line:
#                             parts = line.split('\t')
#                             for part in parts:
#                                 p = part.strip()
#                                 if ',' in p:
#                                     name, url = p.split(',', 1)
#                                     extra = ''
#                                     if '$' in url:
#                                         url, extra = url.split('$', 1)
#                                     data.append([name.strip(), url.strip(), extra.strip()])
#                         else:
#                             if ',' in line:
#                                 name, url = line.split(',', 1)
#                                 extra = ''
#                                 if '$' in url:
#                                     url, extra = url.split('$', 1)
#                                 data.append([name.strip(), url.strip(), extra.strip()])
#             except Exception as e:
#                 logger.error(f"è®¢é˜…æºè¯»å–å¤±è´¥: {line} - {e}")
#         logger.info(f"è®¢é˜…æºå·²å¤„ç†å®Œæ¯•")
#     return data

# def deduplicate(data):
#     df = pd.DataFrame(data, columns=['name', 'url', 'extra'])
#     #df['url'] = df['url'].str.replace('\r', '', regex=False).str.replace('\n', '', regex=False).str.replace(',', '', regex=False).str.replace('"', '', regex=False).str.replace("'", '', regex=False)
#     #df['name'] = df['name'].str.replace('\r', '', regex=False).str.replace('\n', '', regex=False).str.replace(',', '', regex=False).str.replace('"', '', regex=False).str.replace("'", '', regex=False)
#     #df['extra'] = df['extra'].str.replace('\r', '', regex=False).str.replace('\n', '', regex=False).str.replace(',', '', regex=False).str.replace('"', '', regex=False).str.replace("'", '', regex=False)
#     df.drop_duplicates(subset=['name', 'url'], keep='first', inplace=True)
#     return df

# def save_df(df, path):
#     df.to_csv(path, index=False, header=False, encoding='utf-8')
    
# def combine_sources():
#     logger.info("å¼€å§‹æ‰§è¡Œæ¨¡å—2ï¼šè¯»å–è®¢é˜…æº")
#     # è¯»å–æœ¬åœ°æº
#     #result_data = read_sources(r'.\config\user_result.txt')
#     result_data = read_sources(os.path.join("config", "user_result.txt"))
#     #local_data = read_sources(r'.\config\localsource.txt')
#     local_data = read_sources(os.path.join("config", "localsource.txt"))
#     #own_data = read_sources(r'.\output\ownsource.txt')
#     own_data = read_sources(os.path.join("output", "ownsource.txt"))
    
#     # è¯»å–åœ¨çº¿æº
#     #net_data = read_subscribe_sources(r'.\config\subscribe.txt')
#     net_data = read_subscribe_sources(os.path.join("config", "subscribe.txt"))
#     net_df = deduplicate(net_data)
#     #save_df(net_df, r'.\output\netsource.txt')
#     text = '\n'.join(net_df.apply(lambda row: f"{row['name']}|{row['url']}|{row['extra']}", axis=1))
#     with open(os.path.join("output", "netsource_log.txt"), 'w', encoding='utf-8') as f:
#         f.write(text)
#     save_df(net_df, os.path.join("output", "netsource.txt"))

    
#     # åˆå¹¶æ‰€æœ‰æº
#     all_data = result_data + local_data + own_data + net_data
#     all_df = deduplicate(all_data)
#     # all_df['combined'] = all_df.apply(
#     # lambda row: f"{row['name']},{row['url']}${row['extra']}",
#     # axis=1
#     # )
#     all_df.drop(columns=['extra'], inplace=True)
#     #save_df(all_df, r'.\output\allsource.txt')
#     save_df(all_df, os.path.join("output", "allsource.txt"))
#     logger.info("æ¨¡å—2æ‰§è¡Œå®Œæ¯•")
# if __name__ == '__main__':

#     combine_sources()

































